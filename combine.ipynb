{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95741256",
   "metadata": {},
   "source": [
    "# Project: Multimodal Counterfactual Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2a921",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e61b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk transformers torch shap matplotlib pandas scikit-learn\n",
    "!pip install librosa numpy torch transformers\n",
    "!pip install av                                 # PyAV (FFmpeg bindings)\n",
    "!pip install transformers                       # Hugging Face Transformers\n",
    "!pip install pytorchvideo                       # PyTorchVideo (video processor)\n",
    "!pip install opencv-python-headless             # OpenCV I/O\n",
    "!pip install shap                               # SHAP for explainability\n",
    "!pip install torch                              # PyTorch backend\n",
    "!pip install matplotlib                         # Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb602277",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcd94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from nltk.corpus import wordnet as wn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import shap\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "import librosa\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2FeatureExtractor, HubertModel\n",
    "import cv2\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a333af32",
   "metadata": {},
   "source": [
    "## Section: Text Counterfactual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # for antonyms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f081ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Part 2: Model & Prediction Function =====\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model      = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "device     = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "pipe = TextClassificationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "LABEL_ORDER = [\"LABEL_0\", \"LABEL_1\", \"LABEL_2\"]\n",
    "LABELS      = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "def predict_proba(texts):\n",
    "    \"\"\"\n",
    "    Given a list of texts, returns an (n_texts x 3) array of [neg, neu, pos] probabilities.\n",
    "    \"\"\"\n",
    "    raw = pipe(texts)\n",
    "    proba = []\n",
    "    for o in raw:\n",
    "        score_map = {d['label']: d['score'] for d in o}\n",
    "        proba.append([score_map[lbl] for lbl in LABEL_ORDER])\n",
    "    return np.array(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Part 3: Original Prediction & SHAP Bar Chart =====\n",
    "# Target sentence\n",
    "text = (\n",
    "    \"I love you\"\n",
    ")\n",
    "\n",
    "# 3.1 Single-example prediction\n",
    "orig_probs = predict_proba([text])[0]\n",
    "orig_lbl   = LABELS[np.argmax(orig_probs)]\n",
    "print(f\"Original prediction: {orig_lbl.upper()}   {dict(zip(LABELS, orig_probs.round(3)))}\")\n",
    "\n",
    "# 3.2 Probability bar chart\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(LABELS, orig_probs)\n",
    "plt.ylim(0,1)\n",
    "plt.title(\"Original sentiment probabilities\")\n",
    "plt.show()\n",
    "\n",
    "# 3.3 SHAP values bar chart for tokens\n",
    "shap.initjs()\n",
    "explainer = shap.Explainer(pipe, output_names=LABELS)\n",
    "sv        = explainer([text])[0]         # single Explanation\n",
    "pred_idx  = np.argmax(orig_probs)\n",
    "# Extract shapley values for each token for the predicted class\n",
    "shap_vals = sv.values[:, pred_idx]       # shape = (n_tokens,)\n",
    "tokens    = list(sv.data)\n",
    "abs_vals  = np.abs(shap_vals)\n",
    "\n",
    "# Bar chart of absolute SHAP values per token\\ nplt.figure(figsize=(10,4))\n",
    "plt.bar(range(len(tokens)), abs_vals)\n",
    "plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
    "plt.ylabel('abs(SHAP value)')\n",
    "plt.title('Token importance (|SHAP|) for predicted class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f57c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Part 4: SHAP Force Plot =====\n",
    "# Local force-plot showing positive/negative contributions\n",
    "# ===== Add JavaScript Initialization =====\n",
    "\n",
    "# Initialize JS visualization code (MUST COME FIRST)\n",
    "shap.initjs()\n",
    "\n",
    "# ===== Then run your visualization code =====\n",
    "shap.force_plot(\n",
    "    base_value=base_value,\n",
    "    shap_values=shap_values,\n",
    "    features=shap_explanation.data,\n",
    "    feature_names=shap_explanation.feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Revised Cell 5: Counterfactual via Masked-LM Antonym =====\n",
    "\n",
    "\n",
    "# 5.1: Extract SHAP explanation & tokens for the predicted class\n",
    "sv_cf = explainer([text])[0]                # SHAP Explanation object\n",
    "shap_arr = sv_cf.values[:, pred_idx]         # SHAP values for pred class\n",
    "tokens_cf = list(sv_cf.data)                 # token list\n",
    "abs_arr = np.abs(shap_arr)\n",
    "\n",
    "# 5.2: Identify the top-impact token (skip subwords starting with ##)\n",
    "max_score = -np.inf\n",
    "idx_max = -1\n",
    "for i, (score, token) in enumerate(zip(abs_arr, tokens_cf)):\n",
    "    # Skip subword tokens (marked with ## in BERT-style tokenization)\n",
    "    if not token.startswith(\"##\") and score > max_score:\n",
    "        max_score = score\n",
    "        idx_max = i\n",
    "# Fallback if all tokens are subwords\n",
    "if idx_max == -1:\n",
    "    idx_max = np.argmax(abs_arr)\n",
    "token_to_change = tokens_cf[idx_max]\n",
    "print(f\"âš  Top-impact token: '{token_to_change}'\")\n",
    "\n",
    "# 5.3: Improved masked-LM antonym generation\n",
    "mask_pipe = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"bert-base-uncased\",\n",
    "    device=-1  # CPU\n",
    ")\n",
    "\n",
    "def get_antonym_ml(word):\n",
    "    \"\"\"Improved antonym generation with better prompt and validation\"\"\"\n",
    "    # Try multiple prompt templates\n",
    "    templates = [\n",
    "        f\"The antonym of {word} is [MASK].\",\n",
    "        f\"{word} is the opposite of [MASK].\",\n",
    "        f\"Opposite word: {word} â†’ [MASK].\"\n",
    "    ]\n",
    "\n",
    "    candidates = []\n",
    "    for prompt in templates:\n",
    "        results = mask_pipe(prompt, top_k=5)\n",
    "        for res in results:\n",
    "            candidate = res[\"token_str\"].strip().lower()\n",
    "            # Basic validation: antonym shouldn't be same as original\n",
    "            if candidate != word.lower():\n",
    "                candidates.append(candidate)\n",
    "\n",
    "    # Return most common valid candidate\n",
    "    if candidates:\n",
    "        return max(set(candidates), key=candidates.count)\n",
    "    else:\n",
    "        return word  # fallback\n",
    "\n",
    "antonym = get_antonym_ml(token_to_change)\n",
    "print(f\"ğŸ’¡ Replacing '{token_to_change}' â†’ '{antonym}'\")\n",
    "\n",
    "# 5.4: Case-insensitive replacement with lowercase antonym\n",
    "cf_tokens = [\n",
    "    antonym if tok.lower() == token_to_change.lower() else tok\n",
    "    for tok in tokens_cf\n",
    "]\n",
    "cf_text = \" \".join(cf_tokens).replace(\" ##\", \"\")  # Clean subword markers\n",
    "\n",
    "# 5.5: Score original vs. counterfactual\n",
    "orig_probs = predict_proba([text])[0]\n",
    "cf_probs = predict_proba([cf_text])[0]\n",
    "\n",
    "print(\"Original probs:      \", dict(zip(LABELS, orig_probs.round(3))))\n",
    "print(\"Counterfactual probs:\", dict(zip(LABELS, cf_probs.round(3))))\n",
    "\n",
    "# 5.6: Plot comparison\n",
    "x = np.arange(len(LABELS))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(x - width/2, orig_probs, width, label=\"Original\")\n",
    "plt.bar(x + width/2, cf_probs, width, label=\"Counterfactual\")\n",
    "plt.xticks(x, LABELS)\n",
    "plt.ylim(0,1)\n",
    "plt.title(f\"Effect of replacing '{token_to_change}' with '{antonym}'\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f635d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Improved Counterfactual with Antonym Focus =====\n",
    "\n",
    "\n",
    "# 5.1: Extract SHAP explanation & tokens\n",
    "sv_cf = explainer([text])[0]\n",
    "shap_arr = sv_cf.values[:, pred_idx]\n",
    "tokens_cf = list(sv_cf.data)\n",
    "abs_arr = np.abs(shap_arr)\n",
    "\n",
    "# 5.2: Find top-impact token (skip subwords)\n",
    "valid_indices = [i for i, tok in enumerate(tokens_cf) if not tok.startswith(\"##\")]\n",
    "if valid_indices:\n",
    "    idx_max = valid_indices[np.argmax(abs_arr[valid_indices])]\n",
    "else:\n",
    "    idx_max = np.argmax(abs_arr)\n",
    "token_to_change = tokens_cf[idx_max]\n",
    "print(f\"âš  Top-impact token: '{token_to_change}'\")\n",
    "\n",
    "# 5.3: Enhanced antonym generation using contrastive prompting\n",
    "mask_pipe = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"roberta-large\",  # More capable model\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "def get_antonym_ml(word):\n",
    "    \"\"\"Improved antonym generation with correct mask token\"\"\"\n",
    "    prompts = [\n",
    "        f\"Question: What is the direct antonym of '{word}'? Answer: <mask>\",\n",
    "        f\"'{word}' â†’ Opposite: <mask>\",\n",
    "        f\"Antonym pair: {word} and <mask>\",\n",
    "        f\"Positive: {word}, Negative: <mask>\",\n",
    "        f\"If {word} means yes, then <mask> means no\",\n",
    "        f\"Direct opposite of {word}: <mask>\",\n",
    "        f\"Not {word} but <mask>\"\n",
    "    ]\n",
    "\n",
    "    candidates = []\n",
    "    for prompt in prompts:\n",
    "        try:\n",
    "            results = mask_pipe(prompt, top_k=5)\n",
    "            for res in results:\n",
    "                candidate = res[\"token_str\"].strip().lower()\n",
    "                if candidate != word.lower() and len(candidate) > 2:\n",
    "                    candidates.append(candidate)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing prompt '{prompt}': {str(e)}\")\n",
    "\n",
    "    if candidates:\n",
    "        # Get most common candidate after basic filtering\n",
    "        counter = Counter(candidates)\n",
    "        for candidate, _ in counter.most_common():\n",
    "            # Additional validation check\n",
    "            val_prompt = f\"Are '{word}' and '{candidate}' opposites? <mask>\"\n",
    "            val_results = mask_pipe(val_prompt, top_k=1)\n",
    "            if \"yes\" not in val_results[0][\"token_str\"].lower():\n",
    "                return candidate\n",
    "        return counter.most_common(1)[0][0]\n",
    "\n",
    "    return word  # Fallback if no candidates found\n",
    "\n",
    "    # Find most frequent valid candidate\n",
    "    if candidates:\n",
    "        counter = Counter(candidates)\n",
    "        for word, _ in counter.most_common():\n",
    "            # Final validation using opposite context\n",
    "            val_prompt = f\"'{word}' is the opposite of '{word}'? [MASK]\"\n",
    "            val_results = mask_pipe(val_prompt)\n",
    "            if \"no\" in val_results[0][\"token_str\"].lower():\n",
    "                return word\n",
    "        return counter.most_common(1)[0][0]\n",
    "    return word  # Fallback\n",
    "\n",
    "antonym = get_antonym_ml(token_to_change)\n",
    "print(f\"ğŸ’¡ Replacing '{token_to_change}' â†’ '{antonym}'\")\n",
    "\n",
    "# 5.4: Case-sensitive replacement\n",
    "cf_tokens = [\n",
    "    antonym if tok == token_to_change else tok\n",
    "    for tok in tokens_cf\n",
    "]\n",
    "cf_text = \"\".join(cf_tokens).replace(\"Ä \", \" \").replace(\"â–\", \" \")  # Fix tokenization\n",
    "\n",
    "# 5.5: Score and compare\n",
    "orig_probs = predict_proba([text])[0]\n",
    "cf_probs = predict_proba([cf_text])[0]\n",
    "\n",
    "print(\"Original probs:      \", dict(zip(LABELS, orig_probs.round(3))))\n",
    "print(\"Counterfactual probs:\", dict(zip(LABELS, cf_probs.round(3))))\n",
    "\n",
    "# 5.6: Visualization\n",
    "plt.figure(figsize=(6,3))\n",
    "x = np.arange(len(LABELS))\n",
    "plt.bar(x - 0.2, orig_probs, 0.4, label='Original')\n",
    "plt.bar(x + 0.2, cf_probs, 0.4, label='Counterfactual')\n",
    "plt.xticks(x, LABELS)\n",
    "plt.legend()\n",
    "plt.title(f\"Replacement: {token_to_change} â†’ {antonym}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9553ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Modified Code: Remove Top SHAP Word =====\n",
    "\n",
    "\n",
    "# 5.1: Extract SHAP explanation & tokens\n",
    "sv_cf = explainer([text])[0]\n",
    "shap_arr = sv_cf.values[:, pred_idx]\n",
    "tokens_cf = list(sv_cf.data)\n",
    "abs_arr = np.abs(shap_arr)\n",
    "\n",
    "# 5.2: Find top-impact token (skip subwords)\n",
    "valid_indices = [i for i, tok in enumerate(tokens_cf) if not tok.startswith(\"##\")]\n",
    "if valid_indices:\n",
    "    idx_max = valid_indices[np.argmax(abs_arr[valid_indices])]\n",
    "else:\n",
    "    idx_max = np.argmax(abs_arr)\n",
    "token_to_remove = tokens_cf[idx_max]\n",
    "print(f\"âš  Top-impact token to remove: '{token_to_remove}'\")\n",
    "\n",
    "# 5.3: Create text with removed token\n",
    "cf_tokens = [tok for tok in tokens_cf if tok != token_to_remove]\n",
    "\n",
    "# Handle different tokenization schemes\n",
    "if hasattr(sv_cf, 'data'):\n",
    "    # For BERT-style tokenization\n",
    "    cf_text = \" \".join(cf_tokens).replace(\" ##\", \"\")\n",
    "else:\n",
    "    # For RoBERTa/GPT-style tokenization\n",
    "    cf_text = \"\".join(cf_tokens).replace(\"Ä \", \" \").replace(\"â–\", \" \").strip()\n",
    "\n",
    "# 5.4: Score and compare\n",
    "orig_probs = predict_proba([text])[0]\n",
    "cf_probs = predict_proba([cf_text])[0]\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Modified text:\", cf_text)\n",
    "print(\"\\nOriginal probs:      \", dict(zip(LABELS, orig_probs.round(3))))\n",
    "print(\"Modified probs:\", dict(zip(LABELS, cf_probs.round(3))))\n",
    "\n",
    "# 5.5: Visualization\n",
    "plt.figure(figsize=(6,3))\n",
    "x = np.arange(len(LABELS))\n",
    "plt.bar(x - 0.2, orig_probs, 0.4, label='Original')\n",
    "plt.bar(x + 0.2, cf_probs, 0.4, label='With Token Removed')\n",
    "plt.xticks(x, LABELS)\n",
    "plt.legend()\n",
    "plt.title(f\"Effect of removing '{token_to_remove}'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2687f6",
   "metadata": {},
   "source": [
    "## Section: Audio Counterfactual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37deaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€Š2) Load & normalize audio\n",
    "audio_path = \"03-01-05-01-01-02-01.wav\"   # â† your file here\n",
    "y, sr = librosa.load(audio_path, sr=16_000)\n",
    "y = y / np.max(np.abs(y))                 # normalize to [â€“1,1]\n",
    "print(f\"Loaded {audio_path}: {len(y)} samples @ {sr} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecbbf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€Š3) Feature-extract with frozen HuBERT\n",
    "fe = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "hubert = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "hubert.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = fe(y, sampling_rate=sr, return_tensors=\"pt\")\n",
    "    input_values = inputs.input_values               # shape (1, seq_len)\n",
    "    attention_mask = inputs.attention_mask\n",
    "    X = hubert(input_values, attention_mask=attention_mask).last_hidden_state\n",
    "    # X: (1, seq_len, hidden_size)\n",
    "\n",
    "print(\"Extracted HuBERT features:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a650a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€Š4) Define BiLSTM head for 3-way sentiment\n",
    "class AudioSentimentBiLSTM(nn.Module):\n",
    "    def __init__(self, in_dim=1024, hidden_dim=256, num_classes=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm    = nn.LSTM(input_size=in_dim, hidden_size=hidden_dim,\n",
    "                               num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc      = nn.Linear(hidden_dim*2, num_classes)\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, in_dim)\n",
    "        lstm_out, _ = self.lstm(x)                 # (batch, seq_len, hidden*2)\n",
    "        rep = lstm_out.mean(dim=1)                 # mean-pool â†’ (batch, hidden*2)\n",
    "        rep = self.dropout(rep)\n",
    "        return self.fc(rep)                        # logits (batch, 3)\n",
    "\n",
    "model = AudioSentimentBiLSTM(in_dim=X.shape[2])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€Š5) Inference\n",
    "with torch.no_grad():\n",
    "    logits = model(X)                               # (1,3)\n",
    "    probs  = torch.softmax(logits, dim=-1).cpu().numpy().flatten()\n",
    "\n",
    "labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "pred_idx = int(probs.argmax())\n",
    "pred_cls = labels[pred_idx]\n",
    "print(\"Sentiment probabilities:\")\n",
    "for lab,p in zip(labels, probs):\n",
    "    print(f\"  {lab:>8} â†’ {p:.3f}\")\n",
    "print(\"Predicted class:\", pred_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€Š6) SHAP explainability over the featureâ€sequence with GradientExplainer\n",
    "# Ensure X requires gradients\n",
    "X.requires_grad_()  # now X.grad will work\n",
    "\n",
    "# Wrap model so SHAP can call it directly on tensor inputs\n",
    "class LogitsWrapper(nn.Module):\n",
    "    def __init__(self, submodel):\n",
    "        super().__init__()\n",
    "        self.sub = submodel\n",
    "    def forward(self, x):\n",
    "        return self.sub(x)\n",
    "\n",
    "wrapped = LogitsWrapper(model)\n",
    "\n",
    "# Use a zeroâ€background with the same shape as X\n",
    "bg = torch.zeros_like(X)         # shape (1, seq_len, hidden)\n",
    "\n",
    "# Switch to GradientExplainer to avoid the additivity assertion\n",
    "ge = shap.GradientExplainer(wrapped, bg)\n",
    "\n",
    "# Compute SHAP for full sequence\n",
    "# returns a list of arrays, one per class, each of shape (1, seq_len, hidden)\n",
    "shap_vals = ge.shap_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€Š7) Aggregate each timeâ€step by summing absolute contributions across hidden dim\n",
    "sv = np.abs(shap_vals[pred_idx]).sum(axis=-1).flatten()  # â†’ (seq_len,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb05f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€Š8) Build time axes\n",
    "audio_dur = len(y) / sr\n",
    "t_feat    = np.linspace(0, audio_dur, sv.shape[0])\n",
    "t_audio   = np.linspace(0, audio_dur, len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61969fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€Š9) Simple counterfactual: scale amplitude by 0.5\n",
    "cf_y = y * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€Š10) Plot SHAP contributions and waveforms\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# SHAP contributions\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(t_feat, sv, linewidth=2)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Aggregate |SHAP|\")\n",
    "plt.title(f\"SHAP contributions â†’ '{pred_cls}'\")\n",
    "\n",
    "# Original vs CF waveform\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(t_audio, y,    label=\"Original\")\n",
    "plt.plot(t_audio, cf_y, label=\"Counterfactual (Ã—0.5)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.legend()\n",
    "plt.title(\"Waveforms\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf9d56",
   "metadata": {},
   "source": [
    "## Section: Video Counterfactual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update                                  # FFmpeg libs for PyAV\n",
    "!apt-get install -y libavdevice-dev libavfilter-dev libavformat-dev libavcodec-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cee3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 1) TAKE VIDEO AS INPUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "VIDEO_IN = \"videoplayback.mp4\"    # put your filename here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 2) EXTRACT FRAMES & SAMPLE TO 100 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def extract_sampled_frames(path, size=(224, 224), num_frames=100):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    idxs = np.linspace(0, total-1, num=num_frames, dtype=int)\n",
    "    frames = []\n",
    "    for i in range(total):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if i in idxs:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, size)\n",
    "            frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "frames = extract_sampled_frames(VIDEO_IN, num_frames=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model + processor (zeroâ€shot)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_proc  = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Sentiment labels for zeroâ€shot\n",
    "sentiment_labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "text_inputs = clip_proc(text=sentiment_labels, return_tensors=\"pt\", padding=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 3) MODEL: ZEROâ€SHOT SENTIMENT PER FRAME â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def predict_frame_sentiment(frame):\n",
    "    # returns raw scores for each label\n",
    "    inputs = clip_proc(images=frame, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        img_emb = clip_model.get_image_features(**inputs)            # (1, dim)\n",
    "        txt_emb = clip_model.get_text_features(**text_inputs)       # (3, dim)\n",
    "    # cosine similarity\n",
    "    scores = (img_emb @ txt_emb.T)[0] / (img_emb.norm() * txt_emb.norm(dim=1))\n",
    "    probs  = torch.softmax(scores, dim=0).cpu().numpy()\n",
    "    return dict(zip(sentiment_labels, probs))\n",
    "\n",
    "# Compute original video probabilities by averaging frames\n",
    "orig_frame_probs = np.array([list(predict_frame_sentiment(f).values()) for f in frames])\n",
    "orig_probs = orig_frame_probs.mean(axis=0)\n",
    "orig_dict  = dict(zip(sentiment_labels, orig_probs))\n",
    "print(\"Original video sentiment:\", orig_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec0824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 4) SHAPLEY VALUES FRAME-BY-FRAME â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Use mean-pixel intensities as proxy features for each frame\n",
    "frame_feats = np.stack([f.mean(axis=(0,1)).flatten() for f in frames])\n",
    "\n",
    "# SHAP explainer: model_predict maps featsâ†’score(â€œpositiveâ€) for simplicity\n",
    "def model_predict(feat_array):\n",
    "    # reconstruct gray frames from feature\n",
    "    gray_frames = [(np.ones((100,100,3))*feat.mean()).astype(np.uint8) for feat in feat_array]\n",
    "    # get probability of 'positive' label\n",
    "    return np.array([predict_frame_sentiment(f)[\"positive\"] for f in gray_frames])\n",
    "\n",
    "explainer = shap.KernelExplainer(model_predict, frame_feats)\n",
    "shap_vals = explainer.shap_values(frame_feats)  # array of shape (n_frames, feat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403522ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 5) PLOT SHAPLEY VALUE GRAPH â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "mean_abs_shap = np.mean(np.abs(shap_vals), axis=1)  # one value per frame\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(mean_abs_shap, label=\"|SHAP| per frame\")\n",
    "plt.xlabel(\"Frame index\")\n",
    "plt.ylabel(\"Mean |SHAP value|\")\n",
    "plt.title(\"Frame-by-Frame Feature Importance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c051f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 6) COUNTERFACTUAL: FRAMES ALREADY EXTRACTED â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# â”€â”€â”€ 7) NOISE: GAUSSIAN & SALT-AND-PEPPER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def add_noise(frames, noise=\"gaussian\", var=0.05):\n",
    "    out = []\n",
    "    for f in frames:\n",
    "        f_norm = f.astype(np.float32) / 255.0\n",
    "        if noise==\"gaussian\":\n",
    "            g = np.random.normal(0, var**0.5, f_norm.shape)\n",
    "            nf = np.clip(f_norm + g, 0,1)\n",
    "        else:\n",
    "            nf = f_norm.copy()\n",
    "            rnd = np.random.rand(*f_norm.shape[:2])\n",
    "            nf[rnd<var/2]       = 0\n",
    "            nf[rnd>1-var/2]     = 1\n",
    "        out.append((nf*255).astype(np.uint8))\n",
    "    return out\n",
    "\n",
    "cf_frames = add_noise(frames, noise=\"gaussian\", var=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f85e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 7b) WRITE COUNTERFACTUAL VIDEO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def write_video(frames, out_file, fps=25):\n",
    "    h,w = frames[0].shape[:2]\n",
    "    writer = cv2.VideoWriter(out_file, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w,h))\n",
    "    for f in frames:\n",
    "        writer.write(cv2.cvtColor(f, cv2.COLOR_RGB2BGR))\n",
    "    writer.release()\n",
    "\n",
    "CF_OUT = \"cf_video.mp4\"\n",
    "write_video(cf_frames, CF_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 8) COMPARE ORIGINAL vs COUNTERFACTUAL SENTIMENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cf_frame_probs = np.array([list(predict_frame_sentiment(f).values()) for f in cf_frames])\n",
    "cf_probs       = cf_frame_probs.mean(axis=0)\n",
    "cf_dict        = dict(zip(sentiment_labels, cf_probs))\n",
    "print(\"Counterfactual video sentiment:\", cf_dict)\n",
    "\n",
    "# Plot comparison\n",
    "x = np.arange(len(sentiment_labels))\n",
    "width = 0.35\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(x-width/2, orig_probs, width, label=\"Original\")\n",
    "plt.bar(x+width/2, cf_probs,   width, label=\"Counterfactual\")\n",
    "plt.xticks(x, sentiment_labels)\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Original vs Counterfactual Sentiment\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2d027",
   "metadata": {
    "id": "tJLzEAZLBzax"
   },
   "source": [
    "# Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenCVâ€™s face detector\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "def get_counterfactual_frames(frames, mode=\"tight\", margin=0.2):\n",
    "    \"\"\"\n",
    "    mode=\"tight\": crop exactly to detected face\n",
    "    mode=\"zoom_out\": expand crop by `margin` fraction to include more background\n",
    "    \"\"\"\n",
    "    cf = []\n",
    "    for f in frames:\n",
    "        gray = cv2.cvtColor(f, cv2.COLOR_RGB2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "        if len(faces) > 0:\n",
    "            x, y, w, h = faces[0]\n",
    "            if mode == \"tight\":\n",
    "                crop = f[y : y + h, x : x + w]\n",
    "            else:  # zoom_out\n",
    "                mx = int(w * margin)\n",
    "                my = int(h * margin)\n",
    "                x1 = max(0, x - mx)\n",
    "                y1 = max(0, y - my)\n",
    "                x2 = min(f.shape[1], x + w + mx)\n",
    "                y2 = min(f.shape[0], y + h + my)\n",
    "                crop = f[y1:y2, x1:x2]\n",
    "        else:\n",
    "            # no face detected, fallback to full frame\n",
    "            crop = f\n",
    "        # resize back to model input size\n",
    "        cf.append(cv2.resize(crop, (224, 224)))\n",
    "    return cf\n",
    "\n",
    "# Generate your counterfactual frames:\n",
    "# For a tight face crop:\n",
    "cf_frames = get_counterfactual_frames(frames, mode=\"tight\")\n",
    "\n",
    "# Or, to zoom out and include more background:\n",
    "# cf_frames = get_counterfactual_frames(frames, mode=\"zoom_out\", margin=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4788d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ then write and predict exactly as before â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "CF_OUT = \"cff_video.mp4\"\n",
    "write_video(cf_frames, CF_OUT)\n",
    "\n",
    "cf_frame_probs = np.array([list(predict_frame_sentiment(f).values()) for f in cf_frames])\n",
    "cf_probs = cf_frame_probs.mean(axis=0)\n",
    "cf_dict = dict(zip(sentiment_labels, cf_probs))\n",
    "\n",
    "print(\"Counterfactual (face-cropped) sentiment:\", cf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c96679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "x = np.arange(len(sentiment_labels))\n",
    "width = 0.35\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(x - width / 2, orig_probs, width, label=\"Original\")\n",
    "plt.bar(x + width / 2, cf_probs, width, label=\"Counterfactual\")\n",
    "plt.xticks(x, sentiment_labels)\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Original vs Counterfactual Sentiment\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db07b5f",
   "metadata": {
    "id": "ZF-IOgb9t2L-"
   },
   "source": [
    "# Color Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f214d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_clothing_color(frames, hue_shift=60):\n",
    "    \"\"\"\n",
    "    Shifts the hue of the entire frame by `hue_shift` degrees in HSV space,\n",
    "    which effectively remaps clothing (and everything else) to a new color.\n",
    "    You can tweak hue_shift (0-180) to target different color changes.\n",
    "    \"\"\"\n",
    "    cf = []\n",
    "    for f in frames:\n",
    "        # Convert to HSV\n",
    "        hsv = cv2.cvtColor(f, cv2.COLOR_RGB2HSV).astype(np.uint16)\n",
    "        h, s, v = cv2.split(hsv)\n",
    "\n",
    "        # Shift hue channel\n",
    "        h = (h + hue_shift) % 180\n",
    "\n",
    "        # Merge and convert back to RGB\n",
    "        hsv_shifted = cv2.merge([h.astype(np.uint8), s.astype(np.uint8), v.astype(np.uint8)])\n",
    "        rgb_shifted = cv2.cvtColor(hsv_shifted, cv2.COLOR_HSV2RGB)\n",
    "        cf.append(rgb_shifted)\n",
    "    return cf\n",
    "\n",
    "# Generate your counterfactual frames by shifting hue by 60Â° (e.g., blueâ†’green)\n",
    "cf_frames = change_clothing_color(frames, hue_shift=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ then write and predict exactly as before â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "CF_OUT = \"cffc_video.mp4\"\n",
    "write_video(cf_frames, CF_OUT)\n",
    "\n",
    "cf_frame_probs = np.array([list(predict_frame_sentiment(f).values()) for f in cf_frames])\n",
    "cf_probs       = cf_frame_probs.mean(axis=0)\n",
    "cf_dict        = dict(zip(sentiment_labels, cf_probs))\n",
    "\n",
    "print(\"Counterfactual (clothing-color-changed) sentiment:\", cf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb71d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plot (same as before)\n",
    "x = np.arange(len(sentiment_labels))\n",
    "width = 0.35\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(x-width/2, orig_probs, width, label=\"Original\")\n",
    "plt.bar(x+width/2, cf_probs,   width, label=\"Counterfactual\")\n",
    "plt.xticks(x, sentiment_labels)\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Original vs Counterfactual Sentiment\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
